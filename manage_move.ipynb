{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING IMAGES ===\n",
      "\n",
      "Kiểm tra thư mục: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/adidas_final\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/adidas_final/ao_adidas\n",
      "    Tìm thấy 8397 ảnh\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/adidas_final/quan_adidas\n",
      "    Tìm thấy 5148 ảnh\n",
      "\n",
      "Kiểm tra thư mục: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/uniqlo_final\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/uniqlo_final/ao_uniqlo\n",
      "    Tìm thấy 924 ảnh\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/uniqlo_final/quan_uniqlo\n",
      "    Tìm thấy 398 ảnh\n",
      "\n",
      "Kiểm tra thư mục: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/yame_final\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/yame_final/ao_yame\n",
      "    Tìm thấy 2120 ảnh\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/yame_final/quan_yame\n",
      "    Tìm thấy 281 ảnh\n",
      "\n",
      "Kiểm tra thư mục: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/nike_final\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/nike_final/ao_nike\n",
      "    Thư mục không tồn tại: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/nike_final/ao_nike\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/nike_final/quan_nike\n",
      "    Thư mục không tồn tại: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/nike_final/quan_nike\n",
      "\n",
      "Kiểm tra thư mục: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/icondenim_final\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/icondenim_final/ao_icondenim\n",
      "    Thư mục không tồn tại: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/icondenim_final/ao_icondenim\n",
      "  Kiểm tra thư mục con: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/icondenim_final/quan_icondenim\n",
      "    Thư mục không tồn tại: /Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive/icondenim_final/quan_icondenim\n",
      "\n",
      "Tổng số ảnh tìm thấy: 17268\n",
      "Danh sách các nhãn và số lượng:\n",
      "- quan_adidas: 5148 ảnh\n",
      "- quan_yame: 281 ảnh\n",
      "- ao_yame: 2120 ảnh\n",
      "- ao_uniqlo: 924 ảnh\n",
      "- ao_adidas: 8397 ảnh\n",
      "- quan_uniqlo: 398 ảnh\n"
     ]
    }
   ],
   "source": [
    "def get_image_paths(base_dir='/Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive'):\n",
    "    \"\"\"Lấy đường dẫn của tất cả ảnh\"\"\"\n",
    "    brands = {\n",
    "        'adidas_final': ['ao_adidas', 'quan_adidas'],\n",
    "        'uniqlo_final': ['ao_uniqlo', 'quan_uniqlo'],\n",
    "        'yame_final': ['ao_yame', 'quan_yame'],\n",
    "        'nike_final': ['ao_nike', 'quan_nike'],\n",
    "        'icondenim_final': ['ao_icondenim', 'quan_icondenim']\n",
    "    }\n",
    "    \n",
    "    image_paths = []\n",
    "    image_labels = []\n",
    "    \n",
    "    print(\"\\n=== LOADING IMAGES ===\")\n",
    "    for brand, categories in brands.items():\n",
    "        brand_path = os.path.join(base_dir, brand)\n",
    "        print(f\"\\nKiểm tra thư mục: {brand_path}\")\n",
    "        \n",
    "        if os.path.exists(brand_path):\n",
    "            for category in categories:\n",
    "                category_path = os.path.join(brand_path, category)\n",
    "                print(f\"  Kiểm tra thư mục con: {category_path}\")\n",
    "                \n",
    "                if os.path.exists(category_path):\n",
    "                    images = [f for f in os.listdir(category_path) \n",
    "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                    print(f\"    Tìm thấy {len(images)} ảnh\")\n",
    "                    \n",
    "                    for img_name in images:\n",
    "                        img_path = os.path.join(category_path, img_name)\n",
    "                        image_paths.append(img_path)\n",
    "                        image_labels.append(category)\n",
    "                else:\n",
    "                    print(f\"    Thư mục không tồn tại: {category_path}\")\n",
    "        else:\n",
    "            print(f\"  Thư mục không tồn tại: {brand_path}\")\n",
    "    \n",
    "    print(f\"\\nTổng số ảnh tìm thấy: {len(image_paths)}\")\n",
    "    print(\"Danh sách các nhãn và số lượng:\")\n",
    "    for label in set(image_labels):\n",
    "        count = image_labels.count(label)\n",
    "        print(f\"- {label}: {count} ảnh\")\n",
    "    \n",
    "    return image_paths, image_labels\n",
    "\n",
    "def create_data_generators(base_dir='/Users/tvq/Documents/Computer Vision/BÁO CÁO CK/archive', batch_size=128, img_size=(224, 224), validation_split=0.2):\n",
    "    \"\"\"Tạo data generators cho training và validation\"\"\"\n",
    "    print(\"\\n=== CREATING DATA GENERATORS ===\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Image size: {img_size}\")\n",
    "    print(f\"Validation split: {validation_split}\")\n",
    "    \n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,  # Chỉ giữ lại augmentation cần thiết\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "    \n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nThông tin về data generators:\")\n",
    "    print(f\"Số lượng classes: {len(train_generator.class_indices)}\")\n",
    "    print(\"Class mapping:\")\n",
    "    for class_name, class_idx in train_generator.class_indices.items():\n",
    "        print(f\"- {class_name}: {class_idx}\")\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "image_paths, image_labels = get_image_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING DATA GENERATORS ===\n",
      "Batch size: 128\n",
      "Image size: (224, 224)\n",
      "Validation split: 0.2\n",
      "Found 23768 images belonging to 5 classes.\n",
      "Found 5939 images belonging to 5 classes.\n",
      "\n",
      "Thông tin về data generators:\n",
      "Số lượng classes: 5\n",
      "Class mapping:\n",
      "- adidas_final: 0\n",
      "- icondenim_final: 1\n",
      "- nike_final: 2\n",
      "- uniqlo_final: 3\n",
      "- yame_final: 4\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_classes, img_size=(224, 224)):\n",
    "    \"\"\"Tạo mô hình MobileNetV2\"\"\"\n",
    "    print(\"\\n=== CREATING MODEL ===\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Input image size: {img_size}\")\n",
    "    \n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*img_size, 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    print(\"\\nModel Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "train_generator, val_generator = create_data_generators()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPILING MODEL ===\n",
      "Learning rate: 0.001\n",
      "Model compiled successfully!\n",
      "\n",
      "=== TRAINING MODEL ===\n",
      "Number of epochs: 20\n",
      "Save path: best_model.h5\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 14:58:25.743197: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 248, in _finite_generator\n",
      "    yield self._standardize_batch(self.py_dataset[i])\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n",
      "    img = image_utils.load_img(\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n",
      "    img = pil_image.open(io.BytesIO(f.read()))\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/PIL/Image.py\", line 3339, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/186\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:25\u001b[0m 7s/step - accuracy: 0.6797 - loss: 0.7478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 14:58:26.196637: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 248, in _finite_generator\n",
      "    yield self._standardize_batch(self.py_dataset[i])\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n",
      "    img = image_utils.load_img(\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n",
      "    img = pil_image.open(io.BytesIO(f.read()))\n",
      "\n",
      "  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/PIL/Image.py\", line 3339, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\nTraceback (most recent call last):\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 248, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/PIL/Image.py\", line 3339, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_61109]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m---> 57\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_generator, val_generator, epochs, save_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     ModelCheckpoint(\n\u001b[1;32m     26\u001b[0m         save_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m ]\n\u001b[0;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\nTraceback (most recent call last):\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 248, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/tvq/Library/Python/3.9/lib/python/site-packages/PIL/Image.py\", line 3339, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x32631f310>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_61109]"
     ]
    }
   ],
   "source": [
    "def compile_model(model, learning_rate=0.001):\n",
    "    \"\"\"Compile model\"\"\"\n",
    "    print(\"\\n=== COMPILING MODEL ===\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Model compiled successfully!\")\n",
    "    return model\n",
    "model = compile_model(model)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_generator, val_generator, epochs=20, save_path='best_model.h5'):\n",
    "    \"\"\"Huấn luyện mô hình\"\"\"\n",
    "    print(\"\\n=== TRAINING MODEL ===\")\n",
    "    print(f\"Number of epochs: {epochs}\")\n",
    "    print(f\"Save path: {save_path}\")\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "history = train_model(model, train_generator, val_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     26\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 28\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Vẽ đồ thị quá trình training\"\"\"\n",
    "    print(\"\\n=== PLOTTING TRAINING HISTORY ===\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPILING MODEL ===\n",
      "Learning rate: 0.001\n",
      "Model compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "def predict_image(model, image_path, img_size=(224, 224)):\n",
    "    \"\"\"Dự đoán một ảnh\"\"\"\n",
    "    print(\"\\n=== PREDICTING IMAGE ===\")\n",
    "    print(f\"Image path: {image_path}\")\n",
    "    \n",
    "    # Load và preprocess ảnh\n",
    "    img = load_img(image_path, target_size=img_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Dự đoán\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    print(\"\\nPrediction results:\")\n",
    "    print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    print(f\"Raw predictions: {predictions[0]}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
